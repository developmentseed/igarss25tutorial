{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9349c8"
   },
   "source": [
    "# Fine-tuning Clay Foundation Model for Land Cover Segmentation\n",
    "\n",
    "Welcome to Tutorial 3! In this hands-on session, you'll learn how to fine-tune the Clay foundation model for land cover segmentation using the Chesapeake Bay dataset.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/developmentseed/igarss25tutorial/blob/main/tut3a_EOFM_finetune.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "- Understand what foundation models are and why they're powerful for Earth observation\n",
    "- Learn how to fine-tune a pre-trained model for semantic segmentation\n",
    "- Apply transfer learning techniques to land cover classification\n",
    "- Work with real satellite imagery and ground truth labels\n",
    "- Evaluate model performance on geospatial data\n",
    "\n",
    "## What You'll Build\n",
    "You'll create a land cover segmentation model that can classify different types of land use (water, forest, urban areas, etc.) from satellite imagery.\n",
    "\n",
    "## Background for Different Audiences\n",
    "\n",
    "### For GIS Professionals üìç\n",
    "- **Foundation models** are like having a universal \"base map\" that understands Earth's features\n",
    "- **Segmentation** is similar to creating detailed land use polygons, but at the pixel level\n",
    "- Think of this as automated land cover classification that can replace manual digitization\n",
    "- The output is similar to creating a detailed land use/land cover (LULC) raster\n",
    "\n",
    "### For Data Analysts üìä  \n",
    "- We're using **transfer learning** - starting with a model already trained on lots of Earth imagery\n",
    "- **Fine-tuning** means adapting this pre-trained model to our specific classification task\n",
    "- This is like taking a general-purpose tool and customizing it for your specific needs\n",
    "- The model learns patterns in pixel values to predict land cover categories\n",
    "\n",
    "### For ML Engineers ü§ñ\n",
    "- Clay is a **Vision Transformer (ViT)** trained on massive Earth observation datasets\n",
    "- We're doing **semantic segmentation** - predicting a class for every pixel\n",
    "- The architecture uses a **frozen encoder** (Clay) + **trainable segmentation head**\n",
    "- We'll use PyTorch Lightning for training orchestration\n",
    "\n",
    "## Dataset Overview\n",
    "The **Chesapeake Bay Land Cover dataset** contains:\n",
    "- **High-resolution aerial imagery** (NAIP - National Agriculture Imagery Program)\n",
    "- **7 land cover classes**: Water, Tree Canopy, Low Vegetation, Barren, Impervious (Roads), Impervious (Other), No Data\n",
    "- **Pixel-level annotations** for supervised learning\n",
    "- **Real-world complexity** with mixed land uses and seasonal variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Clay Segmentation Architecture Works\n",
    "\n",
    "The `Segmentor` class combines two key components:\n",
    "\n",
    "### 1. **Frozen Clay Encoder** üßä\n",
    "- Pre-trained on millions of Earth observation images\n",
    "- Extracts rich feature representations from input imagery  \n",
    "- **Frozen** = weights don't change during fine-tuning (saves compute!)\n",
    "- Acts like a \"universal feature extractor\" for Earth imagery\n",
    "\n",
    "### 2. **Trainable Segmentation Head** üéØ  \n",
    "- Takes Clay's feature maps and upsamples them to original image size\n",
    "- Uses **convolution + pixel shuffle** operations for efficient upsampling\n",
    "- **Only this part gets trained** - much faster than training from scratch!\n",
    "\n",
    "**Key Parameters:**\n",
    "- `num_classes (int)`: Number of land cover classes to predict (7 for Chesapeake)\n",
    "- `ckpt_path (str)`: Path to the pre-trained Clay model weights\n",
    "\n",
    "**Why This Approach Works:**\n",
    "- ‚úÖ **Faster training**: Only train the small segmentation head\n",
    "- ‚úÖ **Less data needed**: Clay already understands Earth imagery patterns  \n",
    "- ‚úÖ **Better performance**: Foundation model knowledge transfers well\n",
    "- ‚úÖ **Cost effective**: Requires fewer computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Chesapeake Bay Dataset ü¶Ä\n",
    "\n",
    "We'll use the **Chesapeake Bay Land Cover dataset** - a high-quality dataset perfect for learning land cover segmentation.\n",
    "\n",
    "### Dataset Citation\n",
    "If you use this dataset in your work, please cite:\n",
    "> Robinson C, Hou L, Malkin K, Soobitsky R, Czawlytko J, Dilkina B, Jojic N.  \n",
    "> Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data.  \n",
    "> Proceedings of the 2019 Conference on Computer Vision and Pattern Recognition (CVPR 2019).\n",
    "\n",
    "### Why This Dataset is Great for Learning:\n",
    "- **High Resolution**: 1-meter pixel resolution aerial imagery\n",
    "- **Multiple Regions**: Covers diverse landscapes in the Chesapeake Bay area\n",
    "- **Expert Annotations**: Ground truth labels created by domain experts\n",
    "- **Real-world Complexity**: Mixed land uses, seasonal variations, and edge cases\n",
    "- **Well-documented**: Extensively used in research with known baselines\n",
    "\n",
    "### Land Cover Classes (7 total):\n",
    "1. **Water** üíß - Rivers, lakes, bays, coastal areas\n",
    "2. **Tree Canopy/Forest** üå≥ - Dense forest areas, large trees\n",
    "3. **Low Vegetation/Fields** üå± - Grass, crops, shrubs, sparse vegetation  \n",
    "4. **Barren Land** üèîÔ∏è - Exposed soil, construction sites, beaches\n",
    "5. **Impervious (Roads)** üõ£Ô∏è - Paved roads, highways, parking lots\n",
    "6. **Impervious (Other)** üè¢ - Buildings, rooftops, other built structures\n",
    "7. **No Data** ‚¨ú - Areas with missing or invalid data\n",
    "\n",
    "More information: [Chesapeake Bay Dataset](https://lila.science/datasets/chesapeakelandcover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "We'll install all required packages for fine-tuning the Clay model. This notebook is optimized for **Google Colab** but works in any Jupyter environment.\n",
    "\n",
    "### What Each Package Does:\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **lightning**: PyTorch Lightning for training orchestration  \n",
    "- **segmentation_models_pytorch**: Pre-built segmentation architectures\n",
    "- **rasterio**: Reading/writing geospatial raster data (GeoTIFF files)\n",
    "- **s5cmd**: Fast, parallel S3 data transfers\n",
    "\n",
    "### Installation Options\n",
    "\n",
    "**Option 1: All at once (recommended for Colab)**\n",
    "```bash\n",
    "pip install torch lightning segmentation_models_pytorch rasterio s5cmd\n",
    "```\n",
    "\n",
    "**Option 2: Individual packages (if you encounter conflicts)**\n",
    "```bash\n",
    "pip install torch\n",
    "pip install lightning  \n",
    "pip install segmentation_models_pytorch\n",
    "pip install rasterio\n",
    "pip install s5cmd\n",
    "```\n",
    "\n",
    "**For Conda users (local environments):**\n",
    "```bash\n",
    "mamba env create --file environment.yml\n",
    "mamba activate claymodel\n",
    "```\n",
    "\n",
    "Let's install everything we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Install Required Packages\n",
    "\n",
    "Run this cell to install all dependencies. This may take 2-3 minutes in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (this may take a few minutes)\n",
    "!pip install torch lightning segmentation_models_pytorch rasterio s5cmd -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Clone the Clay Model Repository\n",
    "\n",
    "We need the Clay model code for training. This downloads the latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d2cc28a"
   },
   "outputs": [],
   "source": [
    "# Clone the Clay model repository\n",
    "!git clone --depth=1 https://github.com/clay-foundation/model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "982de526"
   },
   "outputs": [],
   "source": [
    "# Navigate to the model directory and check contents\n",
    "%cd model\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêç Add Clay Model to Python Path\n",
    "\n",
    "This makes the Clay model modules available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ef49390"
   },
   "outputs": [],
   "source": [
    "# Add the claymodel directory to Python path so we can import modules\n",
    "import sys\n",
    "sys.path.append(\"./claymodel\")\n",
    "\n",
    "# Import key modules we'll use for training\n",
    "from claymodel.finetune.segment.chesapeake_datamodule import ChesapeakeDataModule\n",
    "from claymodel.finetune.segment.chesapeake_model import ChesapeakeSegmentor\n",
    "\n",
    "print(\"‚úÖ Clay model modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed44c8b7"
   },
   "source": [
    "## üì• Download Training Data\n",
    "\n",
    "We'll download a subset of the Chesapeake Bay dataset for training. The full dataset is ~100GB, so we're using a small sample for this tutorial.\n",
    "\n",
    "### What We're Downloading:\n",
    "- **`*_lc.tif`**: Land cover label images (ground truth)\n",
    "- **`*_naip-new.tif`**: NAIP aerial imagery (input images)\n",
    "- **Training data**: From New York region, 2013\n",
    "- **Validation data**: Separate set for evaluating model performance\n",
    "\n",
    "### About s5cmd:\n",
    "`s5cmd` is a high-performance tool for transferring data from AWS S3. It's much faster than standard AWS CLI for large datasets.\n",
    "\n",
    "**Note**: Download may take 5-10 minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855da283"
   },
   "outputs": [],
   "source": [
    "# Create directory structure for our data\n",
    "!mkdir -p data/cvpr/files/train data/cvpr/files/val\n",
    "\n",
    "# Download training data (subset from NY region)\n",
    "print(\"üì• Downloading training data...\")\n",
    "!s5cmd \\\n",
    "    --no-sign-request \\\n",
    "    cp \\\n",
    "    --include \"m_42076*_lc.tif\" \\\n",
    "    --include \"m_42076*_naip-new.tif\" \\\n",
    "    \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/lcmcvpr2019/cvpr_chesapeake_landcover/ny_1m_2013_extended-debuffered-train_tiles/*\" \\\n",
    "    data/cvpr/files/train/\n",
    "\n",
    "print(\"‚úÖ Training data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "438b652d"
   },
   "outputs": [],
   "source": [
    "# Download validation data (complete validation set)\n",
    "print(\"üì• Downloading validation data...\")\n",
    "!s5cmd \\\n",
    "    --no-sign-request \\\n",
    "    cp \\\n",
    "    --include \"*_lc.tif\" \\\n",
    "    --include \"*_naip-new.tif\" \\\n",
    "    \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/lcmcvpr2019/cvpr_chesapeake_landcover/ny_1m_2013_extended-debuffered-val_tiles/*\" \\\n",
    "    data/cvpr/files/val/\n",
    "\n",
    "print(\"‚úÖ Validation data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc1c1418"
   },
   "source": [
    "### ‚úÖ Verify Downloaded Data\n",
    "\n",
    "Let's check what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1f60b0f"
   },
   "outputs": [],
   "source": [
    "# Check what files we downloaded\n",
    "print(\"üìä Validation data files:\")\n",
    "!ls data/cvpr/files/val | head -10\n",
    "\n",
    "print(f\"\\nüìà Total files in validation: {len(!ls data/cvpr/files/val)} files\")\n",
    "print(f\"üìà Total files in training: {len(!ls data/cvpr/files/train)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb647e98"
   },
   "source": [
    "## üîÑ Data Preprocessing\n",
    "\n",
    "The downloaded GeoTIFF files are large (typically 1000x1000 pixels or more). For efficient training, we need to:\n",
    "\n",
    "1. **Split into smaller chips**: Break large images into 224x224 pixel tiles\n",
    "2. **Organize directory structure**: Separate images and labels into proper folders  \n",
    "3. **Create train/val splits**: Ensure no data leakage between training and validation\n",
    "\n",
    "### Why 224x224 chips?\n",
    "- **Memory efficiency**: Fits in GPU memory for training\n",
    "- **Standard size**: Common input size for vision models\n",
    "- **Balanced coverage**: Good trade-off between context and computational efficiency\n",
    "\n",
    "### What the preprocessing script does:\n",
    "- Reads large GeoTIFF files \n",
    "- Splits them into 224x224 pixel chips\n",
    "- Saves chips as individual image files\n",
    "- Maintains spatial alignment between imagery and labels\n",
    "- Creates proper directory structure for PyTorch Lightning\n",
    "\n",
    "**Note**: This step may take 5-10 minutes to process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c025afbc"
   },
   "outputs": [],
   "source": [
    "# Clean up any existing processed data to ensure fresh start\n",
    "!rm -rf data/cvpr/ny/\n",
    "print(\"üßπ Cleaned up existing processed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6138b86"
   },
   "source": [
    "### üîß Run Data Preprocessing\n",
    "\n",
    "This converts the large GeoTIFF files into training-ready 224x224 image chips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "022d91bf"
   },
   "outputs": [],
   "source": [
    "# Run the preprocessing script\n",
    "# Args: input_dir output_dir chip_size\n",
    "print(\"üîÑ Processing data into 224x224 chips...\")\n",
    "!python claymodel/finetune/segment/preprocess_data.py data/cvpr/files data/cvpr/ny 224\n",
    "print(\"‚úÖ Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94a40240"
   },
   "source": [
    "### üìä Check Processed Data\n",
    "\n",
    "Let's verify our preprocessing worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea8cb4e4"
   },
   "outputs": [],
   "source": [
    "# Check the directory structure and count files\n",
    "!echo \"üìÅ Directory structure:\"\n",
    "!ls -la data/cvpr/ny/\n",
    "\n",
    "!echo -e \"\\nüìä Data counts:\"\n",
    "!echo \"Validation labels: $(ls data/cvpr/ny/val/labels | wc -l) files\"  \n",
    "!echo \"Validation chips: $(ls data/cvpr/ny/val/chips | wc -l) files\"\n",
    "!echo \"Training labels: $(ls data/cvpr/ny/train/labels | wc -l) files\"\n",
    "!echo \"Training chips: $(ls data/cvpr/ny/train/chips | wc -l) files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5d94818"
   },
   "source": [
    "## üèóÔ∏è Download Pre-trained Clay Model\n",
    "\n",
    "Now we need the **pre-trained Clay foundation model**. Think of this as downloading a \"universal Earth imagery expert\" that already understands features like vegetation, water, and built structures.\n",
    "\n",
    "### About the Clay Model:\n",
    "- **Version 1.5**: Latest stable version  \n",
    "- **Size**: ~400MB (this is normal for foundation models!)\n",
    "- **Training**: Trained on millions of satellite/aerial images\n",
    "- **Format**: PyTorch Lightning checkpoint (.ckpt file)\n",
    "\n",
    "### What Makes Clay Special:\n",
    "- üåç **Global coverage**: Trained on imagery from around the world\n",
    "- üõ∞Ô∏è **Multi-sensor**: Works with different satellite/aerial platforms  \n",
    "- üéØ **Transfer learning ready**: Designed to be fine-tuned for specific tasks\n",
    "- ‚ö° **Efficient**: Optimized for both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba79809f"
   },
   "outputs": [],
   "source": [
    "# Create checkpoints directory and download Clay model\n",
    "!mkdir -p checkpoints\n",
    "\n",
    "print(\"‚¨áÔ∏è Downloading Clay v1.5 model (this may take a few minutes)...\")\n",
    "!wget -O checkpoints/clay-v1.5.ckpt https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\n",
    "\n",
    "print(\"‚úÖ Clay model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad56c805"
   },
   "source": [
    "### ‚úÖ Verify Model Download\n",
    "\n",
    "Let's check the downloaded model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47e7775f"
   },
   "outputs": [],
   "source": [
    "# Verify the model was downloaded correctly\n",
    "!ls -lh checkpoints/\n",
    "print(f\"‚úÖ Clay model size: {!du -h checkpoints/clay-v1.5.ckpt | !cut -f1} - looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bfcd009"
   },
   "source": [
    "## ‚öôÔ∏è Training Configuration\n",
    "\n",
    "Before training, let's examine the configuration file that controls all the training parameters. Understanding these settings helps you adapt the model for your own projects.\n",
    "\n",
    "### What's in the Config File:\n",
    "- **Data paths**: Where to find training/validation data\n",
    "- **Model settings**: Architecture choices and hyperparameters  \n",
    "- **Training params**: Learning rate, batch size, number of epochs\n",
    "- **Hardware settings**: GPU usage, mixed precision training\n",
    "- **Logging**: Where to save results and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9984b88f"
   },
   "outputs": [],
   "source": [
    "# Let's look at the training configuration\n",
    "print(\"üìã Training Configuration:\")\n",
    "!cat configs/segment_chesapeake.yaml\n",
    "\n",
    "print(\"\\nüí° Key Settings Explained:\")\n",
    "print(\"- lr: 1e-5 (learning rate - how fast the model learns)\")  \n",
    "print(\"- batch_size: 16 (number of images processed together)\")\n",
    "print(\"- max_epochs: 50 (maximum training iterations)\")\n",
    "print(\"- precision: bf16-mixed (faster training with minimal accuracy loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29bd5d39"
   },
   "source": [
    "### üìù Understanding the Configuration\n",
    "\n",
    "The config file uses YAML format - a human-readable way to specify settings. Here's what each section does:\n",
    "\n",
    "- **data**: Paths to training and validation data\n",
    "- **model**: Architecture and learning parameters  \n",
    "- **trainer**: Hardware settings and training duration\n",
    "- **callbacks**: When to save models and how to monitor progress\n",
    "- **logger**: Where to save training logs and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60a61291"
   },
   "source": [
    "## üöÄ Model Training Setup\n",
    "\n",
    "Now we'll set up the training pipeline using PyTorch Lightning. This approach separates data handling from model training, making the code cleaner and more maintainable.\n",
    "\n",
    "### Training Components:\n",
    "\n",
    "1. **DataModule**: Handles loading and preprocessing of images\n",
    "2. **Model**: The Clay encoder + segmentation head  \n",
    "3. **Trainer**: Orchestrates the training process\n",
    "\n",
    "### Key Benefits of This Approach:\n",
    "- ‚úÖ **Reproducible**: Same setup works across different environments\n",
    "- ‚úÖ **Scalable**: Easy to train on single GPU or multiple GPUs  \n",
    "- ‚úÖ **Maintainable**: Clean separation of concerns\n",
    "- ‚úÖ **Flexible**: Easy to modify individual components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "519f802a"
   },
   "source": [
    "### üìä Initialize Data Module\n",
    "\n",
    "The DataModule handles all data operations - loading images, applying transforms, creating batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c7e49e3"
   },
   "outputs": [],
   "source": [
    "# Initialize the data module with our processed data\n",
    "print(\"üìä Setting up data module...\")\n",
    "\n",
    "dm = ChesapeakeDataModule(\n",
    "    train_chip_dir=\"data/cvpr/ny/train/chips/\",      # Training images\n",
    "    train_label_dir=\"data/cvpr/ny/train/labels/\",    # Training labels  \n",
    "    val_chip_dir=\"data/cvpr/ny/val/chips/\",          # Validation images\n",
    "    val_label_dir=\"data/cvpr/ny/val/labels/\",        # Validation labels\n",
    "    metadata_path=\"configs/metadata.yaml\",           # Data normalization info\n",
    "    batch_size=16,                                   # Images per training batch\n",
    "    num_workers=8,                                   # Parallel data loading processes  \n",
    "    platform=\"naip\",                                 # Image type (NAIP aerial imagery)\n",
    ")\n",
    "\n",
    "# Prepare the data loaders\n",
    "dm.setup()\n",
    "print(\"‚úÖ Data module ready!\")\n",
    "print(f\"üìà Training batches: {len(dm.train_dataloader())}\")\n",
    "print(f\"üìä Validation batches: {len(dm.val_dataloader())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8c5a9b5"
   },
   "source": [
    "### ü§ñ Initialize the Model\n",
    "\n",
    "Now we create our segmentation model - Clay encoder + segmentation head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5df0ed89"
   },
   "outputs": [],
   "source": [
    "# Initialize the segmentation model\n",
    "print(\"ü§ñ Setting up segmentation model...\")\n",
    "\n",
    "model = ChesapeakeSegmentor(\n",
    "    num_classes=7,                              # 7 land cover classes\n",
    "    ckpt_path=\"checkpoints/clay-v1.5.ckpt\",    # Pre-trained Clay model\n",
    "    lr=1e-5,                                    # Learning rate (conservative for fine-tuning)\n",
    "    wd=0.05,                                    # Weight decay (regularization)\n",
    "    b1=0.9,                                     # Adam optimizer beta1  \n",
    "    b2=0.95,                                    # Adam optimizer beta2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model initialized!\")\n",
    "print(f\"üßä Clay encoder: FROZEN (saves compute)\")\n",
    "print(f\"üéØ Segmentation head: TRAINABLE (learns land cover patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070f9eea"
   },
   "source": [
    "### ‚ö° Setup the Trainer\n",
    "\n",
    "The Trainer handles the training loop, GPU usage, and checkpointing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7703a0ee"
   },
   "outputs": [],
   "source": [
    "# Import the Trainer\n",
    "from lightning import Trainer\n",
    "\n",
    "print(\"‚ö° Setting up trainer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5106207f"
   },
   "outputs": [],
   "source": [
    "# Configure the trainer for our training session\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",                    # Automatically detect GPU/CPU\n",
    "    devices=1,                            # Use 1 device (GPU if available)  \n",
    "    num_nodes=1,                          # Single machine training\n",
    "    precision=\"bf16-mixed\",               # Mixed precision (faster training)\n",
    "    log_every_n_steps=5,                  # Log metrics every 5 training steps\n",
    "    max_epochs=1,                         # Train for 1 epoch (demo purposes)\n",
    "    accumulate_grad_batches=1,            # No gradient accumulation\n",
    "    default_root_dir=\"checkpoints/segment\", # Where to save checkpoints\n",
    "    fast_dev_run=False,                   # Full training (not debugging mode)\n",
    "    num_sanity_val_steps=0,               # Skip validation sanity check\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured!\")\n",
    "print(f\"üéØ Will train for {trainer.max_epochs} epoch(s)\")\n",
    "print(f\"üíæ Checkpoints saved to: {trainer.default_root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f0ccb40"
   },
   "source": [
    "### üèÅ Start Training!\n",
    "\n",
    "Everything is set up - let's train the model! This will:\n",
    "\n",
    "1. **Load batches** of images and labels\n",
    "2. **Forward pass**: Run images through Clay encoder + segmentation head  \n",
    "3. **Compute loss**: Compare predictions to ground truth labels\n",
    "4. **Backward pass**: Calculate gradients for the segmentation head\n",
    "5. **Update weights**: Improve the segmentation head parameters\n",
    "6. **Validate**: Test performance on validation data\n",
    "7. **Save checkpoint**: Store the trained model\n",
    "\n",
    "**Expected time**: ~5-10 minutes for 1 epoch (depending on hardware)\n",
    "\n",
    "**What to watch for**:\n",
    "- Training loss should decrease over time\n",
    "- Validation metrics should improve\n",
    "- No out-of-memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e719df9"
   },
   "outputs": [],
   "source": [
    "# Start the training process!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"üìä Watch the progress below:\")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(\"üìÅ Check the checkpoints directory for your trained model\")\n",
    "print(\"‚û°Ô∏è Next: Run the inference notebook to see predictions!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNR7shfz8QYTJ76+LqkLSft",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f991cfee5b45b6845dd1ed546f2b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "055bcb3145934ce981516b35303bc652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5819e92f85b47bcb430695fdffa0d7e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_01f991cfee5b45b6845dd1ed546f2b0e",
      "value": "‚Äá63/63‚Äá[04:27&lt;00:00,‚Äá‚Äá0.24it/s]"
     }
    },
    "1ccb42bc6c364b7392dd6cd198ee9766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8954b000b3a423baf6f57b6d66c8ce4",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e96da0097d8a4bc4b5baa46c6b627021",
      "value": 63
     }
    },
    "1e9fb43410a5494c9bb23556bc2e3b44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "2606f336d9ce487e97b464a7ca5667b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "2f2be8a1fa1848a9bf0d1a12036d17c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "308313ec25864ba9a62d5e7b53018505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d65c7174c746426099a583e6df40a7f4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_979b681021794c67a275a8e24e466282",
      "value": "Epoch‚Äá1:‚Äá‚Äá32%"
     }
    },
    "31d8003099ba485c9786493013210155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edd0a95d995c4282a7dbb4455eb69e6c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_93886706e3e44f37afef57c6a360348a",
      "value": "‚Äá20/63‚Äá[01:40&lt;03:36,‚Äá‚Äá0.20it/s,‚Äáv_num=0,‚Äátrain/loss_step=0.203,‚Äátrain/iou_step=0.752,‚Äátrain/f1_step=0.845,‚Äával/loss_step=0.0938,‚Äával/iou_step=0.901,‚Äával/f1_step=0.947,‚Äával/loss_epoch=0.158,‚Äával/iou_epoch=0.831,‚Äával/f1_epoch=0.897,‚Äátrain/loss_epoch=0.340,‚Äátrain/iou_epoch=0.726,‚Äátrain/f1_epoch=0.811]"
     }
    },
    "447f1e98679644ce8a0ba5d049653234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_980083c34e924210b439d8e5f670293b",
       "IPY_MODEL_1ccb42bc6c364b7392dd6cd198ee9766",
       "IPY_MODEL_055bcb3145934ce981516b35303bc652"
      ],
      "layout": "IPY_MODEL_1e9fb43410a5494c9bb23556bc2e3b44"
     }
    },
    "643c6f27f9f34745a1875c628a128d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aa597bc97674e2499d1325d584badb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_308313ec25864ba9a62d5e7b53018505",
       "IPY_MODEL_d59c964b0d7940318fbeacb7448e7cc6",
       "IPY_MODEL_31d8003099ba485c9786493013210155"
      ],
      "layout": "IPY_MODEL_2606f336d9ce487e97b464a7ca5667b9"
     }
    },
    "8e2bdc0ac29a466ab540235ecd1b9466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93886706e3e44f37afef57c6a360348a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "979b681021794c67a275a8e24e466282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "980083c34e924210b439d8e5f670293b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de6ab9352f3f435abe7449189db82787",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2f2be8a1fa1848a9bf0d1a12036d17c7",
      "value": "Validation‚ÄáDataLoader‚Äá0:‚Äá100%"
     }
    },
    "a5819e92f85b47bcb430695fdffa0d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d59c964b0d7940318fbeacb7448e7cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_643c6f27f9f34745a1875c628a128d77",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e2bdc0ac29a466ab540235ecd1b9466",
      "value": 20
     }
    },
    "d65c7174c746426099a583e6df40a7f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6ab9352f3f435abe7449189db82787": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8954b000b3a423baf6f57b6d66c8ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e96da0097d8a4bc4b5baa46c6b627021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "edd0a95d995c4282a7dbb4455eb69e6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
