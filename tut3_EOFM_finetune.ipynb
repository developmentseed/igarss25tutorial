{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f9349c8"
   },
   "source": [
    "# Fine-tuning Clay Foundation Model for Land Cover Segmentation\n",
    "\n",
    "Welcome to Tutorial 3! In this hands-on session, you'll learn how to fine-tune the Clay foundation model for land cover segmentation using the Chesapeake Bay dataset.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/developmentseed/igarss25tutorial/blob/main/tut3_EOFM_finetune.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "- Understand what foundation models are and why they're powerful for Earth observation\n",
    "- Learn how to fine-tune a pre-trained model for semantic segmentation\n",
    "- Apply transfer learning techniques to land cover classification\n",
    "- Work with real satellite imagery and ground truth labels\n",
    "- Evaluate model performance on geospatial data\n",
    "\n",
    "## What You'll Build\n",
    "You'll create a land cover segmentation model that can classify different types of land use (water, forest, urban areas, etc.) from satellite imagery.\n",
    "\n",
    "## Background for Different Audiences\n",
    "\n",
    "### For GIS Professionals üìç\n",
    "- **Foundation models** are like having a universal \"base map\" that understands Earth's features\n",
    "- **Segmentation** is similar to creating detailed land use polygons, but at the pixel level\n",
    "- Think of this as automated land cover classification that can replace manual digitization\n",
    "- The output is similar to creating a detailed land use/land cover (LULC) raster\n",
    "\n",
    "### For Data Analysts üìä  \n",
    "- We're using **transfer learning** - starting with a model already trained on lots of Earth imagery\n",
    "- **Fine-tuning** means adapting this pre-trained model to our specific classification task\n",
    "- This is like taking a general-purpose tool and customizing it for your specific needs\n",
    "- The model learns patterns in pixel values to predict land cover categories\n",
    "\n",
    "### For ML Engineers ü§ñ\n",
    "- Clay is a **Vision Transformer (ViT)** trained on massive Earth observation datasets\n",
    "- We're doing **semantic segmentation** - predicting a class for every pixel\n",
    "- The architecture uses a **frozen encoder** (Clay) + **trainable segmentation head**\n",
    "- We'll use PyTorch Lightning for training orchestration\n",
    "\n",
    "## Dataset Overview\n",
    "The **Chesapeake Bay Land Cover dataset** contains:\n",
    "- **High-resolution aerial imagery** (NAIP - National Agriculture Imagery Program)\n",
    "- **7 land cover classes**: Water, Tree Canopy, Low Vegetation, Barren, Impervious (Roads), Impervious (Other), No Data\n",
    "- **Pixel-level annotations** for supervised learning\n",
    "- **Real-world complexity** with mixed land uses and seasonal variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Clay Segmentation Architecture Works\n",
    "\n",
    "The `Segmentor` class combines two key components:\n",
    "\n",
    "### 1. **Frozen Clay Encoder** üßä\n",
    "- Pre-trained on millions of Earth observation images\n",
    "- Extracts rich feature representations from input imagery  \n",
    "- **Frozen** = weights don't change during fine-tuning (saves compute!)\n",
    "- Acts like a \"universal feature extractor\" for Earth imagery\n",
    "\n",
    "### 2. **Trainable Segmentation Head** üéØ  \n",
    "- Takes Clay's feature maps and upsamples them to original image size\n",
    "- Uses **convolution + pixel shuffle** operations for efficient upsampling\n",
    "- **Only this part gets trained** - much faster than training from scratch!\n",
    "\n",
    "**Key Parameters:**\n",
    "- `num_classes (int)`: Number of land cover classes to predict (7 for Chesapeake)\n",
    "- `ckpt_path (str)`: Path to the pre-trained Clay model weights\n",
    "\n",
    "**Why This Approach Works:**\n",
    "- ‚úÖ **Faster training**: Only train the small segmentation head\n",
    "- ‚úÖ **Less data needed**: Clay already understands Earth imagery patterns  \n",
    "- ‚úÖ **Better performance**: Foundation model knowledge transfers well\n",
    "- ‚úÖ **Cost effective**: Requires fewer computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Chesapeake Bay Dataset ü¶Ä\n",
    "\n",
    "We'll use the **Chesapeake Bay Land Cover dataset** - a high-quality dataset perfect for learning land cover segmentation.\n",
    "\n",
    "### Dataset Citation\n",
    "If you use this dataset in your work, please cite:\n",
    "> Robinson C, Hou L, Malkin K, Soobitsky R, Czawlytko J, Dilkina B, Jojic N.  \n",
    "> Large Scale High-Resolution Land Cover Mapping with Multi-Resolution Data.  \n",
    "> Proceedings of the 2019 Conference on Computer Vision and Pattern Recognition (CVPR 2019).\n",
    "\n",
    "### Why This Dataset is Great for Learning:\n",
    "- **High Resolution**: 1-meter pixel resolution aerial imagery\n",
    "- **Multiple Regions**: Covers diverse landscapes in the Chesapeake Bay area\n",
    "- **Expert Annotations**: Ground truth labels created by domain experts\n",
    "- **Real-world Complexity**: Mixed land uses, seasonal variations, and edge cases\n",
    "- **Well-documented**: Extensively used in research with known baselines\n",
    "\n",
    "### Land Cover Classes (7 total):\n",
    "1. **Water** üíß - Rivers, lakes, bays, coastal areas\n",
    "2. **Tree Canopy/Forest** üå≥ - Dense forest areas, large trees\n",
    "3. **Low Vegetation/Fields** üå± - Grass, crops, shrubs, sparse vegetation  \n",
    "4. **Barren Land** üèîÔ∏è - Exposed soil, construction sites, beaches\n",
    "5. **Impervious (Roads)** üõ£Ô∏è - Paved roads, highways, parking lots\n",
    "6. **Impervious (Other)** üè¢ - Buildings, rooftops, other built structures\n",
    "7. **No Data** ‚¨ú - Areas with missing or invalid data\n",
    "\n",
    "More information: [Chesapeake Bay Dataset](https://lila.science/datasets/chesapeakelandcover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "We'll install all required packages for fine-tuning the Clay model. This notebook is optimized for **Google Colab** but works in any Jupyter environment.\n",
    "\n",
    "### What Each Package Does:\n",
    "- **torch**: PyTorch deep learning framework\n",
    "- **lightning**: PyTorch Lightning for training orchestration  \n",
    "- **segmentation_models_pytorch**: Pre-built segmentation architectures\n",
    "- **rasterio**: Reading/writing geospatial raster data (GeoTIFF files)\n",
    "- **s5cmd**: Fast, parallel S3 data transfers\n",
    "\n",
    "### Installation Options\n",
    "\n",
    "**Option 1: All at once (recommended for Colab)**\n",
    "```bash\n",
    "pip install torch lightning segmentation_models_pytorch rasterio s5cmd\n",
    "```\n",
    "\n",
    "**Option 2: Individual packages (if you encounter conflicts)**\n",
    "```bash\n",
    "pip install torch\n",
    "pip install lightning  \n",
    "pip install segmentation_models_pytorch\n",
    "pip install rasterio\n",
    "pip install s5cmd\n",
    "```\n",
    "\n",
    "**For [`uv`](https://docs.astral.sh/uv/) users (local environments):**\n",
    "```bash\n",
    "uv sync --locked\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "Let's install everything we need:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Install Required Packages\n",
    "\n",
    "Run this cell to install all dependencies. This may take 2-3 minutes in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (this may take a few minutes)\n",
    "!pip install torch lightning segmentation_models_pytorch rasterio s5cmd -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÇ Clone the Clay Model Repository\n",
    "\n",
    "We need the Clay model code for training. This downloads the latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d2cc28a"
   },
   "outputs": [],
   "source": [
    "# Clone the Clay model repository\n",
    "!git clone --depth=1 https://github.com/clay-foundation/model.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "982de526"
   },
   "outputs": [],
   "source": [
    "# Navigate to the model directory and check contents\n",
    "%cd model\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêç Add Clay Model to Python Path\n",
    "\n",
    "This makes the Clay model modules available for import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ef49390"
   },
   "outputs": [],
   "source": [
    "# Add the claymodel directory to Python path so we can import modules\n",
    "import sys\n",
    "sys.path.append(\"./claymodel\")\n",
    "\n",
    "# Import key modules we'll use for training\n",
    "from claymodel.finetune.segment.chesapeake_datamodule import ChesapeakeDataModule\n",
    "from claymodel.finetune.segment.chesapeake_model import ChesapeakeSegmentor\n",
    "\n",
    "print(\"‚úÖ Clay model modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed44c8b7"
   },
   "source": [
    "## üì• Download Training Data\n",
    "\n",
    "We'll download a subset of the Chesapeake Bay dataset for training. The full dataset is ~100GB, so we're using a small sample for this tutorial.\n",
    "\n",
    "### What We're Downloading:\n",
    "- **`*_lc.tif`**: Land cover label images (ground truth)\n",
    "- **`*_naip-new.tif`**: NAIP aerial imagery (input images)\n",
    "- **Training data**: From New York region, 2013\n",
    "- **Validation data**: Separate set for evaluating model performance\n",
    "\n",
    "### About s5cmd:\n",
    "`s5cmd` is a high-performance tool for transferring data from AWS S3. It's much faster than standard AWS CLI for large datasets.\n",
    "\n",
    "**Note**: Download may take 5-10 minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "855da283"
   },
   "outputs": [],
   "source": [
    "# Create directory structure for our data\n",
    "!mkdir -p data/cvpr/files/train data/cvpr/files/val\n",
    "\n",
    "# Download training data (subset from NY region)\n",
    "print(\"üì• Downloading training data...\")\n",
    "!s5cmd \\\n",
    "    --no-sign-request \\\n",
    "    cp \\\n",
    "    --include \"m_42076*_lc.tif\" \\\n",
    "    --include \"m_42076*_naip-new.tif\" \\\n",
    "    \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/lcmcvpr2019/cvpr_chesapeake_landcover/ny_1m_2013_extended-debuffered-train_tiles/*\" \\\n",
    "    data/cvpr/files/train/\n",
    "\n",
    "print(\"‚úÖ Training data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "438b652d"
   },
   "outputs": [],
   "source": [
    "# Download validation data (complete validation set)\n",
    "print(\"üì• Downloading validation data...\")\n",
    "!s5cmd \\\n",
    "    --no-sign-request \\\n",
    "    cp \\\n",
    "    --include \"*_lc.tif\" \\\n",
    "    --include \"*_naip-new.tif\" \\\n",
    "    \"s3://us-west-2.opendata.source.coop/agentmorris/lila-wildlife/lcmcvpr2019/cvpr_chesapeake_landcover/ny_1m_2013_extended-debuffered-val_tiles/*\" \\\n",
    "    data/cvpr/files/val/\n",
    "\n",
    "print(\"‚úÖ Validation data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc1c1418"
   },
   "source": [
    "### ‚úÖ Verify Downloaded Data\n",
    "\n",
    "Let's check what we downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what files we downloaded\n",
    "print(\"üìä Train data files:\")\n",
    "!ls data/cvpr/files/train | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1f60b0f"
   },
   "outputs": [],
   "source": [
    "print(\"üìä Validation data files:\")\n",
    "!ls data/cvpr/files/val | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb647e98"
   },
   "source": [
    "## üîÑ Data Preprocessing\n",
    "\n",
    "The downloaded GeoTIFF files are large (typically 1000x1000 pixels or more). For efficient training, we need to:\n",
    "\n",
    "1. **Split into smaller chips**: Break large images into 224x224 pixel tiles\n",
    "2. **Organize directory structure**: Separate images and labels into proper folders  \n",
    "3. **Create train/val splits**: Ensure no data leakage between training and validation\n",
    "\n",
    "### Why 224x224 chips?\n",
    "- **Memory efficiency**: Fits in GPU memory for training\n",
    "- **Standard size**: Common input size for vision models\n",
    "- **Balanced coverage**: Good trade-off between context and computational efficiency\n",
    "\n",
    "### What the preprocessing script does:\n",
    "- Reads large GeoTIFF files \n",
    "- Splits them into 224x224 pixel chips\n",
    "- Saves chips as individual image files\n",
    "- Maintains spatial alignment between imagery and labels\n",
    "- Creates proper directory structure for PyTorch Lightning\n",
    "\n",
    "**Note**: This step may take 5-10 minutes to process all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c025afbc"
   },
   "outputs": [],
   "source": [
    "# Clean up any existing processed data to ensure fresh start\n",
    "!rm -rf data/cvpr/ny/\n",
    "print(\"üßπ Cleaned up existing processed data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6138b86"
   },
   "source": [
    "### üîß Run Data Preprocessing\n",
    "\n",
    "This converts the large GeoTIFF files into training-ready 224x224 image chips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "022d91bf"
   },
   "outputs": [],
   "source": [
    "# Run the preprocessing script\n",
    "# Args: input_dir output_dir chip_size\n",
    "print(\"üîÑ Processing data into 224x224 chips...\")\n",
    "!python claymodel/finetune/segment/preprocess_data.py data/cvpr/files data/cvpr/ny 224\n",
    "print(\"‚úÖ Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94a40240"
   },
   "source": [
    "### üìä Check Processed Data\n",
    "\n",
    "Let's verify our preprocessing worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea8cb4e4"
   },
   "outputs": [],
   "source": [
    "# Check the directory structure and count files\n",
    "!echo \"üìÅ Directory structure:\"\n",
    "!ls -la data/cvpr/ny/\n",
    "\n",
    "!echo -e \"\\nüìä Data counts:\"\n",
    "!echo \"Validation labels: $(ls data/cvpr/ny/val/labels | wc -l) files\"  \n",
    "!echo \"Validation chips: $(ls data/cvpr/ny/val/chips | wc -l) files\"\n",
    "!echo \"Training labels: $(ls data/cvpr/ny/train/labels | wc -l) files\"\n",
    "!echo \"Training chips: $(ls data/cvpr/ny/train/chips | wc -l) files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5d94818"
   },
   "source": [
    "## üèóÔ∏è Download Pre-trained Clay Model\n",
    "\n",
    "Now we need the **pre-trained Clay foundation model**. Think of this as downloading a \"universal Earth imagery expert\" that already understands features like vegetation, water, and built structures.\n",
    "\n",
    "### About the Clay Model:\n",
    "- **Version 1.5**: Latest stable version  \n",
    "- **Size**: ~400MB (this is normal for foundation models!)\n",
    "- **Training**: Trained on millions of satellite/aerial images\n",
    "- **Format**: PyTorch Lightning checkpoint (.ckpt file)\n",
    "\n",
    "### What Makes Clay Special:\n",
    "- üåç **Global coverage**: Trained on imagery from around the world\n",
    "- üõ∞Ô∏è **Multi-sensor**: Works with different satellite/aerial platforms  \n",
    "- üéØ **Transfer learning ready**: Designed to be fine-tuned for specific tasks\n",
    "- ‚ö° **Efficient**: Optimized for both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ba79809f"
   },
   "outputs": [],
   "source": [
    "# Create checkpoints directory and download Clay model\n",
    "!mkdir -p checkpoints\n",
    "\n",
    "print(\"‚¨áÔ∏è Downloading Clay v1.5 model (this may take a few minutes)...\")\n",
    "!wget -O checkpoints/clay-v1.5.ckpt https://huggingface.co/made-with-clay/Clay/resolve/main/v1.5/clay-v1.5.ckpt\n",
    "\n",
    "print(\"‚úÖ Clay model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad56c805"
   },
   "source": [
    "### ‚úÖ Verify Model Download\n",
    "\n",
    "Let's check the downloaded model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47e7775f"
   },
   "outputs": [],
   "source": [
    "# Verify the model was downloaded correctly\n",
    "!ls -lh checkpoints/\n",
    "!du -h checkpoints/clay-v1.5.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bfcd009"
   },
   "source": [
    "## ‚öôÔ∏è Training Configuration\n",
    "\n",
    "Before training, let's examine the configuration file that controls all the training parameters. Understanding these settings helps you adapt the model for your own projects.\n",
    "\n",
    "### What's in the Config File:\n",
    "- **Data paths**: Where to find training/validation data\n",
    "- **Model settings**: Architecture choices and hyperparameters  \n",
    "- **Training params**: Learning rate, batch size, number of epochs\n",
    "- **Hardware settings**: GPU usage, mixed precision training\n",
    "- **Logging**: Where to save results and checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9984b88f"
   },
   "outputs": [],
   "source": [
    "# Let's look at the training configuration\n",
    "print(\"üìã Training Configuration:\")\n",
    "!cat configs/segment_chesapeake.yaml\n",
    "\n",
    "print(\"\\nüí° Key Settings Explained:\")\n",
    "print(\"- lr: 1e-5 (learning rate - how fast the model learns)\")  \n",
    "print(\"- batch_size: 16 (number of images processed together)\")\n",
    "print(\"- max_epochs: 50 (maximum training iterations)\")\n",
    "print(\"- precision: bf16-mixed (faster training with minimal accuracy loss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29bd5d39"
   },
   "source": [
    "### üìù Understanding the Configuration\n",
    "\n",
    "The config file uses YAML format - a human-readable way to specify settings. Here's what each section does:\n",
    "\n",
    "- **data**: Paths to training and validation data\n",
    "- **model**: Architecture and learning parameters  \n",
    "- **trainer**: Hardware settings and training duration\n",
    "- **callbacks**: When to save models and how to monitor progress\n",
    "- **logger**: Where to save training logs and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60a61291"
   },
   "source": [
    "## üöÄ Model Training Setup\n",
    "\n",
    "Now we'll set up the training pipeline using PyTorch Lightning. This approach separates data handling from model training, making the code cleaner and more maintainable.\n",
    "\n",
    "### Training Components:\n",
    "\n",
    "1. **DataModule**: Handles loading and preprocessing of images\n",
    "2. **Model**: The Clay encoder + segmentation head  \n",
    "3. **Trainer**: Orchestrates the training process\n",
    "\n",
    "### Key Benefits of This Approach:\n",
    "- ‚úÖ **Reproducible**: Same setup works across different environments\n",
    "- ‚úÖ **Scalable**: Easy to train on single GPU or multiple GPUs  \n",
    "- ‚úÖ **Maintainable**: Clean separation of concerns\n",
    "- ‚úÖ **Flexible**: Easy to modify individual components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "519f802a"
   },
   "source": [
    "### üìä Initialize Data Module\n",
    "\n",
    "The DataModule handles all data operations - loading images, applying transforms, creating batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c7e49e3"
   },
   "outputs": [],
   "source": [
    "# Initialize the data module with our processed data\n",
    "print(\"üìä Setting up data module...\")\n",
    "\n",
    "dm = ChesapeakeDataModule(\n",
    "    train_chip_dir=\"data/cvpr/ny/train/chips/\",      # Training images\n",
    "    train_label_dir=\"data/cvpr/ny/train/labels/\",    # Training labels  \n",
    "    val_chip_dir=\"data/cvpr/ny/val/chips/\",          # Validation images\n",
    "    val_label_dir=\"data/cvpr/ny/val/labels/\",        # Validation labels\n",
    "    metadata_path=\"configs/metadata.yaml\",           # Data normalization info\n",
    "    batch_size=16,                                   # Images per training batch\n",
    "    num_workers=8,                                   # Parallel data loading processes  \n",
    "    platform=\"naip\",                                 # Image type (NAIP aerial imagery)\n",
    ")\n",
    "\n",
    "# Prepare the data loaders\n",
    "dm.setup()\n",
    "print(\"‚úÖ Data module ready!\")\n",
    "print(f\"üìà Training batches: {len(dm.train_dataloader())}\")\n",
    "print(f\"üìä Validation batches: {len(dm.val_dataloader())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8c5a9b5"
   },
   "source": [
    "### ü§ñ Initialize the Model\n",
    "\n",
    "Now we create our segmentation model - Clay encoder + segmentation head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5df0ed89"
   },
   "outputs": [],
   "source": [
    "# Initialize the segmentation model\n",
    "print(\"ü§ñ Setting up segmentation model...\")\n",
    "\n",
    "model = ChesapeakeSegmentor(\n",
    "    num_classes=7,                              # 7 land cover classes\n",
    "    ckpt_path=\"checkpoints/clay-v1.5.ckpt\",    # Pre-trained Clay model\n",
    "    lr=1e-5,                                    # Learning rate (conservative for fine-tuning)\n",
    "    wd=0.05,                                    # Weight decay (regularization)\n",
    "    b1=0.9,                                     # Adam optimizer beta1  \n",
    "    b2=0.95,                                    # Adam optimizer beta2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model initialized!\")\n",
    "print(f\"üßä Clay encoder: FROZEN (saves compute)\")\n",
    "print(f\"üéØ Segmentation head: TRAINABLE (learns land cover patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "070f9eea"
   },
   "source": [
    "### ‚ö° Setup the Trainer\n",
    "\n",
    "The Trainer handles the training loop, GPU usage, and checkpointing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7703a0ee"
   },
   "outputs": [],
   "source": [
    "# Import the Trainer\n",
    "from lightning import Trainer\n",
    "\n",
    "print(\"‚ö° Setting up trainer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5106207f"
   },
   "outputs": [],
   "source": [
    "# Configure the trainer for our training session\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",                    # Automatically detect GPU/CPU\n",
    "    devices=1,                            # Use 1 device (GPU if available)  \n",
    "    num_nodes=1,                          # Single machine training\n",
    "    precision=\"bf16-mixed\",               # Mixed precision (faster training)\n",
    "    log_every_n_steps=5,                  # Log metrics every 5 training steps\n",
    "    max_epochs=1,                         # Train for 1 epoch (demo purposes)\n",
    "    accumulate_grad_batches=1,            # No gradient accumulation\n",
    "    default_root_dir=\"checkpoints/segment\", # Where to save checkpoints\n",
    "    fast_dev_run=False,                   # Full training (not debugging mode)\n",
    "    num_sanity_val_steps=0,               # Skip validation sanity check\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer configured!\")\n",
    "print(f\"üéØ Will train for {trainer.max_epochs} epoch(s)\")\n",
    "print(f\"üíæ Checkpoints saved to: {trainer.default_root_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f0ccb40"
   },
   "source": [
    "### üèÅ Start Training!\n",
    "\n",
    "Everything is set up - let's train the model! This will:\n",
    "\n",
    "1. **Load batches** of images and labels\n",
    "2. **Forward pass**: Run images through Clay encoder + segmentation head  \n",
    "3. **Compute loss**: Compare predictions to ground truth labels\n",
    "4. **Backward pass**: Calculate gradients for the segmentation head\n",
    "5. **Update weights**: Improve the segmentation head parameters\n",
    "6. **Validate**: Test performance on validation data\n",
    "7. **Save checkpoint**: Store the trained model\n",
    "\n",
    "**Expected time**: ~5-10 minutes for 1 epoch (depending on hardware)\n",
    "\n",
    "**What to watch for**:\n",
    "- Training loss should decrease over time\n",
    "- Validation metrics should improve\n",
    "- No out-of-memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e719df9"
   },
   "outputs": [],
   "source": [
    "# Start the training process!\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"üìä Watch the progress below:\")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "\n",
    "print(\"\\nüéâ Training complete!\")\n",
    "print(\"üìÅ Check the checkpoints directory for your trained model\")\n",
    "print(\"‚û°Ô∏è Next: Lets run the inference to see predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference and Results Visualization \n",
    "\n",
    "Welcome to the **inference and visualization** part of Tutorial - Here you'll see your fine-tuned Clay model in action, making predictions on real satellite imagery.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to load a trained segmentation model for inference\n",
    "- Techniques for visualizing model predictions\n",
    "- How to interpret land cover segmentation results\n",
    "- Methods for comparing predictions with ground truth\n",
    "- Best practices for model evaluation\n",
    "\n",
    "## What We'll Do\n",
    "1. **Load the trained model** from the previous notebook\n",
    "2. **Prepare validation data** for testing\n",
    "3. **Run inference** to generate predictions  \n",
    "4. **Visualize results** with color-coded land cover maps\n",
    "5. **Compare predictions** with ground truth labels\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### For GIS Professionals üìç\n",
    "- **Inference**: Using your trained model to classify new imagery\n",
    "- **Visualization**: Creating interpretable land cover maps from model outputs\n",
    "- Think of this as automated feature extraction and classification\n",
    "- Results can be exported as GeoTIFF files for use in GIS software\n",
    "\n",
    "### For Data Analysts üìä\n",
    "- **Model evaluation**: Assessing how well our model performs\n",
    "- **Visual validation**: Checking predictions against known ground truth\n",
    "- **Pattern recognition**: Understanding what the model learned vs. missed\n",
    "- **Quality assessment**: Identifying areas for model improvement\n",
    "\n",
    "### For ML Engineers ü§ñ\n",
    "- **Inference pipeline**: Loading checkpoints and running forward passes\n",
    "- **Post-processing**: Converting logits to class predictions\n",
    "- **Batch processing**: Efficient handling of multiple images\n",
    "- **Model interpretation**: Understanding model behavior through visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have access to our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5677,
     "status": "ok",
     "timestamp": 1753698661576,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "FoerYdfrk115"
   },
   "outputs": [],
   "source": [
    "# Verify we have the necessary files\n",
    "print(\"üìÅ Current directory contents:\")\n",
    "!ls -la\n",
    "\n",
    "print(\"\\nüîç Checking for trained model...\")\n",
    "!ls -la checkpoints/segment/lightning_logs/*/checkpoints/ 2>/dev/null || echo \"‚ùå No trained model found - please run the training notebook first!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Import Required Libraries\n",
    "\n",
    "Let's import all the tools we need for inference and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7774,
     "status": "ok",
     "timestamp": 1753704792942,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "79cQtuG8zjHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "üîß PyTorch version: 2.7.0\n",
      "üéÆ GPU available: No (using CPU)\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Additional utilities  \n",
    "from einops import rearrange\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéÆ GPU available: {'Yes' if torch.cuda.is_available() else 'No (using CPU)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration and File Paths\n",
    "\n",
    "Let's define all the paths and parameters we'll need. These should match what you used in the training notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1753705499470,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "MzaVKezS7myo"
   },
   "outputs": [],
   "source": [
    "# File paths and configuration\n",
    "CHESAPEAKE_CHECKPOINT_PATH = \"checkpoints/segment/lightning_logs/version_0/checkpoints/epoch=0-step=63.ckpt\"\n",
    "CLAY_CHECKPOINT_PATH = \"checkpoints/clay-v1.5.ckpt\"\n",
    "METADATA_PATH = \"configs/metadata.yaml\"\n",
    "\n",
    "# Data directories\n",
    "TRAIN_CHIP_DIR = \"data/cvpr/ny/train/chips/\"\n",
    "TRAIN_LABEL_DIR = \"data/cvpr/ny/train/labels/\"\n",
    "VAL_CHIP_DIR = \"data/cvpr/ny/val/chips/\"\n",
    "VAL_LABEL_DIR = \"data/cvpr/ny/val/labels/\"\n",
    "\n",
    "# Data loading parameters\n",
    "BATCH_SIZE = 32          # Process 32 images at once (larger batch for inference)\n",
    "NUM_WORKERS = 1          # Single worker to avoid issues in Colab\n",
    "PLATFORM = \"naip\"        # NAIP aerial imagery platform\n",
    "\n",
    "print(\"üìã Configuration loaded:\")\n",
    "print(f\"   üéØ Model checkpoint: {CHESAPEAKE_CHECKPOINT_PATH}\")\n",
    "print(f\"   üß† Clay model: {CLAY_CHECKPOINT_PATH}\")\n",
    "print(f\"   üìä Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   üì∑ Platform: {PLATFORM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Helper Functions\n",
    "\n",
    "Let's define functions to handle model loading, data preparation, inference, and visualization. Breaking these into functions makes the code more organized and reusable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1753704799919,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "Z6bn-IR877Cd"
   },
   "outputs": [],
   "source": [
    "def get_model(chesapeake_checkpoint_path, clay_checkpoint_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Load the trained segmentation model from checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        chesapeake_checkpoint_path: Path to our trained model\n",
    "        clay_checkpoint_path: Path to the Clay foundation model  \n",
    "        metadata_path: Path to data normalization metadata\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded model in evaluation mode\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Loading trained model...\")\n",
    "    \n",
    "    model = ChesapeakeSegmentor.load_from_checkpoint(\n",
    "        checkpoint_path=chesapeake_checkpoint_path,\n",
    "        metadata_path=metadata_path,\n",
    "        ckpt_path=clay_checkpoint_path,\n",
    "    )\n",
    "    \n",
    "    # Set to evaluation mode (disables dropout, batch norm training mode, etc.)\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753704800371,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "zuRqtBkW8Byf"
   },
   "outputs": [],
   "source": [
    "def get_data(train_chip_dir, train_label_dir, val_chip_dir, val_label_dir, \n",
    "             metadata_path, batch_size, num_workers, platform):\n",
    "    \"\"\"\n",
    "    Set up data loading for inference.\n",
    "    \n",
    "    Args:\n",
    "        Various paths and parameters for data loading\n",
    "        \n",
    "    Returns:\n",
    "        batch: A batch of validation data\n",
    "        metadata: Data normalization and class information\n",
    "    \"\"\"\n",
    "    print(\"üìä Setting up data loader...\")\n",
    "    \n",
    "    # Create data module (same as training, but we only need validation data)\n",
    "    dm = ChesapeakeDataModule(\n",
    "        train_chip_dir=train_chip_dir,\n",
    "        train_label_dir=train_label_dir,  \n",
    "        val_chip_dir=val_chip_dir,\n",
    "        val_label_dir=val_label_dir,\n",
    "        metadata_path=metadata_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        platform=platform,\n",
    "    )\n",
    "    \n",
    "    # Setup the data module\n",
    "    dm.setup(stage=\"fit\")\n",
    "    \n",
    "    # Get one batch of validation data for visualization\n",
    "    val_dl = iter(dm.val_dataloader())\n",
    "    batch = next(val_dl)\n",
    "    \n",
    "    print(f\"‚úÖ Data loaded - batch contains {batch['pixels'].shape[0]} images\")\n",
    "    print(f\"üìè Image shape: {list(batch['pixels'].shape[1:])}\")\n",
    "    \n",
    "    return batch, dm.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753704800929,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "fNRlWKy18Fg6"
   },
   "outputs": [],
   "source": [
    "def run_prediction(model, batch):\n",
    "    \"\"\"\n",
    "    Run inference on a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained segmentation model\n",
    "        batch: Batch of input images\n",
    "        \n",
    "    Returns:\n",
    "        outputs: Model predictions (probabilities for each class)\n",
    "    \"\"\"\n",
    "    print(\"üîÆ Running inference...\")\n",
    "    \n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through the model\n",
    "        outputs = model(batch)\n",
    "    \n",
    "    # Upsample predictions to match original image size (256x256)\n",
    "    # The model outputs smaller feature maps that need to be upsampled\n",
    "    outputs = F.interpolate(\n",
    "        outputs, \n",
    "        size=(256, 256),           # Target size\n",
    "        mode=\"bilinear\",           # Smooth interpolation\n",
    "        align_corners=False        # PyTorch default\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Inference complete - predictions shape: {list(outputs.shape)}\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1753704801906,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "QHb7oR7T8IhK"
   },
   "outputs": [],
   "source": [
    "def denormalize_images(normalized_images, means, stds):\n",
    "    \"\"\"\n",
    "    Convert normalized images back to viewable format.\n",
    "    \n",
    "    During training, images are normalized (mean=0, std=1) for better model performance.  \n",
    "    For visualization, we need to reverse this normalization.\n",
    "    \n",
    "    Args:\n",
    "        normalized_images: Normalized image tensors\n",
    "        means: Mean values used for normalization\n",
    "        stds: Standard deviation values used for normalization\n",
    "        \n",
    "    Returns:\n",
    "        denormalized_images: Images in 0-255 range for display\n",
    "    \"\"\"\n",
    "    means = np.array(means).reshape(1, -1, 1, 1)\n",
    "    stds = np.array(stds).reshape(1, -1, 1, 1)\n",
    "    \n",
    "    # Reverse normalization: multiply by std, then add mean\n",
    "    denormalized_images = normalized_images * stds + means\n",
    "    \n",
    "    # Convert to 0-255 range for display\n",
    "    return denormalized_images.astype(np.uint8)\n",
    "\n",
    "\n",
    "def post_process(batch, outputs, metadata):\n",
    "    \"\"\"\n",
    "    Convert model outputs and inputs into visualization-ready format.\n",
    "    \n",
    "    Args:\n",
    "        batch: Original batch of data\n",
    "        outputs: Model prediction probabilities\n",
    "        metadata: Data normalization info\n",
    "        \n",
    "    Returns:\n",
    "        images: RGB images ready for display\n",
    "        labels: Ground truth segmentation maps\n",
    "        preds: Predicted segmentation maps\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Post-processing results...\")\n",
    "    \n",
    "    # Convert prediction probabilities to class predictions\n",
    "    # argmax selects the class with highest probability for each pixel\n",
    "    preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    # Extract ground truth labels\n",
    "    labels = batch[\"label\"].detach().cpu().numpy()\n",
    "    \n",
    "    # Extract normalized pixel values\n",
    "    pixels = batch[\"pixels\"].detach().cpu().numpy()\n",
    "    \n",
    "    # Get normalization parameters for this platform (NAIP)\n",
    "    means = list(metadata[\"naip\"].bands.mean.values())\n",
    "    stds = list(metadata[\"naip\"].bands.std.values())\n",
    "    \n",
    "    # Denormalize images for display\n",
    "    norm_pixels = denormalize_images(pixels, means, stds)\n",
    "    \n",
    "    # Rearrange from (batch, channels, height, width) to (batch, height, width, channels)\n",
    "    # This is the format matplotlib expects for RGB images\n",
    "    images = rearrange(norm_pixels[:, :3, :, :], \"b c h w -> b h w c\")\n",
    "    \n",
    "    print(f\"‚úÖ Post-processing complete\")\n",
    "    print(f\"üìä Processed {len(images)} images\")\n",
    "    \n",
    "    return images, labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1753705487159,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "uUnqSEQ48LpA"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(images, labels, preds):\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization of results.\n",
    "    \n",
    "    Shows original images, ground truth labels, and model predictions\n",
    "    in an easy-to-compare grid format.\n",
    "    \n",
    "    Args:\n",
    "        images: RGB aerial images\n",
    "        labels: Ground truth segmentation maps  \n",
    "        preds: Model predicted segmentation maps\n",
    "    \"\"\"\n",
    "    print(\"üé® Creating visualization...\")\n",
    "    \n",
    "    # Define colors for each land cover class\n",
    "    # These colors are chosen to be intuitive and visually distinct\n",
    "    colors = [\n",
    "        (0/255, 0/255, 255/255, 1),         # Deep Blue for water üíß\n",
    "        (34/255, 139/255, 34/255, 1),       # Forest Green for tree canopy üå≥\n",
    "        (154/255, 205/255, 50/255, 1),      # Yellow Green for low vegetation üå±\n",
    "        (210/255, 180/255, 140/255, 1),     # Tan for barren land üèîÔ∏è\n",
    "        (169/255, 169/255, 169/255, 1),     # Dark Gray for impervious (other) üè¢\n",
    "        (105/255, 105/255, 105/255, 1),     # Dim Gray for impervious (road) üõ£Ô∏è\n",
    "        (255/255, 255/255, 255/255, 1),     # White for no data ‚¨ú\n",
    "    ]\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Create a large figure to show all comparisons\n",
    "    fig, axes = plt.subplots(12, 8, figsize=(16, 24))\n",
    "    fig.suptitle('üåç Land Cover Segmentation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot in three rows: Images, Ground Truth, Predictions\n",
    "    plot_data(axes, images, row_offset=0, title=\"üì∑ Original Image\")\n",
    "    plot_data(axes, labels, row_offset=1, title=\"üéØ Ground Truth\", cmap=cmap, vmin=0, vmax=6)\n",
    "    plot_data(axes, preds, row_offset=2, title=\"ü§ñ Model Prediction\", cmap=cmap, vmin=0, vmax=6)\n",
    "    \n",
    "    # Add a legend explaining the color scheme\n",
    "    add_legend(fig, cmap)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Visualization complete!\")\n",
    "\n",
    "\n",
    "def plot_data(axes, data, row_offset, title=None, cmap=None, vmin=None, vmax=None):\n",
    "    \"\"\"Helper function to plot a row of data in the grid.\"\"\"\n",
    "    for i, item in enumerate(data):\n",
    "        if i >= 24:  # Only show first 24 images (3 rows of 8)\n",
    "            break\n",
    "            \n",
    "        row = row_offset + (i // 8) * 3\n",
    "        col = i % 8\n",
    "        \n",
    "        axes[row, col].imshow(item, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axes[row, col].axis(\"off\")\n",
    "        \n",
    "        # Add row titles\n",
    "        if title and col == 0:\n",
    "            axes[row, col].set_ylabel(title, rotation=0, fontsize=12, \n",
    "                                    fontweight='bold', ha='right', va='center')\n",
    "\n",
    "\n",
    "def add_legend(fig, cmap):\n",
    "    \"\"\"Add a color legend explaining the land cover classes.\"\"\"\n",
    "    class_names = [\n",
    "        \"üíß Water\",\n",
    "        \"üå≥ Tree Canopy\", \n",
    "        \"üå± Low Vegetation\",\n",
    "        \"üèîÔ∏è Barren Land\",\n",
    "        \"üè¢ Impervious (Other)\",\n",
    "        \"üõ£Ô∏è Impervious (Roads)\", \n",
    "        \"‚¨ú No Data\"\n",
    "    ]\n",
    "    \n",
    "    # Create legend patches\n",
    "    import matplotlib.patches as mpatches\n",
    "    patches = [mpatches.Patch(color=cmap.colors[i], label=class_names[i]) \n",
    "               for i in range(len(class_names))]\n",
    "    \n",
    "    # Add legend to the figure\n",
    "    fig.legend(handles=patches, loc='center', bbox_to_anchor=(0.5, 0.02), \n",
    "               ncol=4, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run the Complete Inference Pipeline\n",
    "\n",
    "Now let's put it all together! We'll load the model, prepare data, run inference, and visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37596,
     "status": "ok",
     "timestamp": 1753705388920,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "59ulQ_6_8O9p"
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = get_model(CHESAPEAKE_CHECKPOINT_PATH, CLAY_CHECKPOINT_PATH, METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 919,
     "status": "ok",
     "timestamp": 1753705512246,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "ENe89qFL8RNl"
   },
   "outputs": [],
   "source": [
    "# Get validation data for testing\n",
    "batch, metadata = get_data(\n",
    "    TRAIN_CHIP_DIR,\n",
    "    TRAIN_LABEL_DIR,\n",
    "    VAL_CHIP_DIR,\n",
    "    VAL_LABEL_DIR,\n",
    "    METADATA_PATH,\n",
    "    BATCH_SIZE,\n",
    "    NUM_WORKERS,\n",
    "    PLATFORM,\n",
    ")\n",
    "\n",
    "# Move data to GPU if available (same device as model)\n",
    "device = next(model.parameters()).device\n",
    "batch = {k: v.to(device) for k, v in batch.items()}\n",
    "print(f\"üì± Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4997,
     "status": "ok",
     "timestamp": 1753705518228,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "EDNX9uql8Z0O"
   },
   "outputs": [],
   "source": [
    "# Run inference on the batch\n",
    "outputs = run_prediction(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1753705519735,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "8yxPaSul8cUK"
   },
   "outputs": [],
   "source": [
    "# Post-process results for visualization\n",
    "images, labels, preds = post_process(batch, outputs, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2918,
     "status": "ok",
     "timestamp": 1753705523671,
     "user": {
      "displayName": "Soumya Ranjan",
      "userId": "07004583440856095975"
     },
     "user_tz": -330
    },
    "id": "SLig0yiy_P4I",
    "outputId": "a5776d39-83a6-408f-a179-1f0f68f3d236"
   },
   "outputs": [],
   "source": [
    "# Create the final visualization\n",
    "plot_predictions(images, labels, preds)\n",
    "\n",
    "print(\"\\nüéâ Inference and visualization complete!\")\n",
    "print(\"\\nüîç What to Look For:\")\n",
    "print(\"   ‚Ä¢ How well does the model identify water bodies?\")\n",
    "print(\"   ‚Ä¢ Are forest areas correctly classified?\") \n",
    "print(\"   ‚Ä¢ Does the model distinguish between different types of impervious surfaces?\")\n",
    "print(\"   ‚Ä¢ Where does the model struggle or make mistakes?\")\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   ‚Ä¢ Try running on more batches to see consistency\")\n",
    "print(\"   ‚Ä¢ Consider additional training epochs for better performance\")\n",
    "print(\"   ‚Ä¢ Experiment with different learning rates or data augmentation\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNR7shfz8QYTJ76+LqkLSft",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f991cfee5b45b6845dd1ed546f2b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "055bcb3145934ce981516b35303bc652": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5819e92f85b47bcb430695fdffa0d7e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_01f991cfee5b45b6845dd1ed546f2b0e",
      "value": "‚Äá63/63‚Äá[04:27&lt;00:00,‚Äá‚Äá0.24it/s]"
     }
    },
    "1ccb42bc6c364b7392dd6cd198ee9766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8954b000b3a423baf6f57b6d66c8ce4",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e96da0097d8a4bc4b5baa46c6b627021",
      "value": 63
     }
    },
    "1e9fb43410a5494c9bb23556bc2e3b44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": "100%"
     }
    },
    "2606f336d9ce487e97b464a7ca5667b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "2f2be8a1fa1848a9bf0d1a12036d17c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "308313ec25864ba9a62d5e7b53018505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d65c7174c746426099a583e6df40a7f4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_979b681021794c67a275a8e24e466282",
      "value": "Epoch‚Äá1:‚Äá‚Äá32%"
     }
    },
    "31d8003099ba485c9786493013210155": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edd0a95d995c4282a7dbb4455eb69e6c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_93886706e3e44f37afef57c6a360348a",
      "value": "‚Äá20/63‚Äá[01:40&lt;03:36,‚Äá‚Äá0.20it/s,‚Äáv_num=0,‚Äátrain/loss_step=0.203,‚Äátrain/iou_step=0.752,‚Äátrain/f1_step=0.845,‚Äával/loss_step=0.0938,‚Äával/iou_step=0.901,‚Äával/f1_step=0.947,‚Äával/loss_epoch=0.158,‚Äával/iou_epoch=0.831,‚Äával/f1_epoch=0.897,‚Äátrain/loss_epoch=0.340,‚Äátrain/iou_epoch=0.726,‚Äátrain/f1_epoch=0.811]"
     }
    },
    "447f1e98679644ce8a0ba5d049653234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_980083c34e924210b439d8e5f670293b",
       "IPY_MODEL_1ccb42bc6c364b7392dd6cd198ee9766",
       "IPY_MODEL_055bcb3145934ce981516b35303bc652"
      ],
      "layout": "IPY_MODEL_1e9fb43410a5494c9bb23556bc2e3b44"
     }
    },
    "643c6f27f9f34745a1875c628a128d77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aa597bc97674e2499d1325d584badb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_308313ec25864ba9a62d5e7b53018505",
       "IPY_MODEL_d59c964b0d7940318fbeacb7448e7cc6",
       "IPY_MODEL_31d8003099ba485c9786493013210155"
      ],
      "layout": "IPY_MODEL_2606f336d9ce487e97b464a7ca5667b9"
     }
    },
    "8e2bdc0ac29a466ab540235ecd1b9466": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "93886706e3e44f37afef57c6a360348a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "979b681021794c67a275a8e24e466282": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "980083c34e924210b439d8e5f670293b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de6ab9352f3f435abe7449189db82787",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2f2be8a1fa1848a9bf0d1a12036d17c7",
      "value": "Validation‚ÄáDataLoader‚Äá0:‚Äá100%"
     }
    },
    "a5819e92f85b47bcb430695fdffa0d7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d59c964b0d7940318fbeacb7448e7cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_643c6f27f9f34745a1875c628a128d77",
      "max": 63,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8e2bdc0ac29a466ab540235ecd1b9466",
      "value": 20
     }
    },
    "d65c7174c746426099a583e6df40a7f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6ab9352f3f435abe7449189db82787": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8954b000b3a423baf6f57b6d66c8ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e96da0097d8a4bc4b5baa46c6b627021": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "edd0a95d995c4282a7dbb4455eb69e6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
